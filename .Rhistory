library(tidyverse)
library(tidytext)
library(stringi)
setwd(getwd())
list.files()
list.files("./data")
list.files("./noticias")
#defino los csv
list.files(path = "./noticias", pattern = "csv")
#defino los csv
archivos <- list.files(path = "./noticias", pattern = "csv")
#levanto todas las noticias en un solo df
for (i in archivos) {
df<-read.csv(i, encoding = "UTF-8")
base<-rbind(base, df)
}
df<-read.csv(paste0("./noticias/", i, encoding = "UTF-8")
base<-rbind(base, df)
}
noticias<-base%>%
mutate(
texto= str_to_lower(texto),
texto= stri_trans_general(texto, "Latin - ASCII"),
texto= str_replace_all(texto,"www\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto,"https\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto, "[[:punct:]]", " "),
texto= str_replace_all(texto, "[[:digit:]]+", " "))%>%
filter(str_detect(texto, "bullrich"))
filtro_palabras<-c(stri_trans_general(stopwords::stopwords("es"), "Latin-ASCII"),
"patricia", "milei", "massa", "bullrich", "javier")
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))
write.table(palabras, file = "palabras.csv", quote = F, row.names = F, sep = ";")
#levanto todas las noticias en un solo df
for (i in archivos) {
#levanto todas las noticias en un solo df
for (i in archivos) {
df<-read.csv(paste0("./noticias/", i, encoding = "UTF-8"))
base<-rbind(base, df)
}
df<-read.csv(paste0("./noticias/", i), encoding = "UTF-8")
#limpio textos de noticias y filtro las que mencionan al candidato
noticias<-base%>%
mutate(
texto= str_to_lower(texto),
texto= stri_trans_general(texto, "Latin - ASCII"),
texto= str_replace_all(texto,"www\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto,"https\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto, "[[:punct:]]", " "),
texto= str_replace_all(texto, "[[:digit:]]+", " "))%>%
filter(str_detect(texto, candidato))
# defino candidato
candidato<-"bullrich"
#limpio textos de noticias y filtro las que mencionan al candidato
noticias<-base%>%
mutate(
texto= str_to_lower(texto),
texto= stri_trans_general(texto, "Latin - ASCII"),
texto= str_replace_all(texto,"www\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto,"https\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto, "[[:punct:]]", " "),
texto= str_replace_all(texto, "[[:digit:]]+", " "))%>%
filter(str_detect(texto, candidato))
#creo un filtro de palabras que no quiero que salgan en el conteo
filtro_palabras<-c(stri_trans_general(stopwords::stopwords("es"), "Latin-ASCII"),
"patricia", "milei", "massa", "bullrich", "javier")
#hago el conteo de palabras
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))
#limpio textos de noticias y filtro las que mencionan al candidato
noticias<-base%>%
mutate(
texto= str_to_lower(texto),
texto= stri_trans_general(texto, "Latin - ASCII"),
texto= str_replace_all(texto,"www\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto,"https\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto, "[[:punct:]]", " "),
texto= str_replace_all(texto, "[[:digit:]]+", " "))%>%
filter(str_detect(texto, candidato))
#creo un df vacío
base<-data.frame()
#levanto todas las noticias en un solo df
for (i in archivos) {
df<-read.csv(paste0("./noticias/", i), encoding = "UTF-8")
base<-rbind(base, df)
}
# defino candidato
candidato<-"bullrich"
#limpio textos de noticias y filtro las que mencionan al candidato
noticias<-base%>%
mutate(
texto= str_to_lower(texto),
texto= stri_trans_general(texto, "Latin - ASCII"),
texto= str_replace_all(texto,"www\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto,"https\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto, "[[:punct:]]", " "),
texto= str_replace_all(texto, "[[:digit:]]+", " "))%>%
filter(str_detect(texto, candidato))
#creo un filtro de palabras que no quiero que salgan en el conteo
filtro_palabras<-c(stri_trans_general(stopwords::stopwords("es"), "Latin-ASCII"),
"patricia", "milei", "massa", "bullrich", "javier",
"anos", "dos")
#hago el conteo de palabras
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))
View(palabras)
write.table(palabras, file = paste0("./resultados/", candidato, "_nubedepalabras_", Sys.Date(),".csv"), quote = F, row.names = F, sep = ";")
#hago el conteo de palabras
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))%>%
slice(150)
write.table(palabras, file = paste0("./resultados/", candidato, "_nubedepalabras_", Sys.Date(),".csv"), quote = F, row.names = F, sep = ";")
#hago el conteo de palabras
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))%>%
slice(1:150)
write.table(palabras, file = paste0("./resultados/", candidato, "_nubedepalabras_", Sys.Date(),".csv"), quote = F, row.names = F, sep = ";")
# defino candidato
candidato<-"massa"
#limpio textos de noticias y filtro las que mencionan al candidato
noticias<-base%>%
mutate(
texto= str_to_lower(texto),
texto= stri_trans_general(texto, "Latin - ASCII"),
texto= str_replace_all(texto,"www\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto,"https\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto, "[[:punct:]]", " "),
texto= str_replace_all(texto, "[[:digit:]]+", " "))%>%
filter(str_detect(texto, candidato))
#creo un filtro de palabras que no quiero que salgan en el conteo
filtro_palabras<-c(stri_trans_general(stopwords::stopwords("es"), "Latin-ASCII"),
"patricia", "milei", "massa", "bullrich", "javier",
"anos", "dos")
#creo un filtro de palabras que no quiero que salgan en el conteo
filtro_palabras<-c(stri_trans_general(stopwords::stopwords("es"), "Latin-ASCII"),
"patricia", "milei", "massa", "bullrich", "javier",
"anos", "dos")
#hago el conteo de palabras
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))%>%
slice(1:150)
write.table(palabras, file = paste0("./resultados/", candidato, "_nubedepalabras_", Sys.Date(),".csv"), quote = F, row.names = F, sep = ";")
# defino candidato
candidato<-"bregman"
#limpio textos de noticias y filtro las que mencionan al candidato
noticias<-base%>%
mutate(
texto= str_to_lower(texto),
texto= stri_trans_general(texto, "Latin - ASCII"),
texto= str_replace_all(texto,"www\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto,"https\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto, "[[:punct:]]", " "),
texto= str_replace_all(texto, "[[:digit:]]+", " "))%>%
filter(str_detect(texto, candidato))
#creo un filtro de palabras que no quiero que salgan en el conteo
filtro_palabras<-c(stri_trans_general(stopwords::stopwords("es"), "Latin-ASCII"),
"patricia", "milei", "massa", "bullrich", "javier",
"anos", "dos")
#hago el conteo de palabras
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))%>%
slice(1:150)
write.table(palabras, file = paste0("./resultados/", candidato, "_nubedepalabras_", Sys.Date(),".csv"), quote = F, row.names = F, sep = ";")
library(tidyverse)
library(tidytext)
library(stringi)
getwd()
getwd()
getwd()
setwd(getwd())
#defino los csv
archivos <- list.files(path = "./noticias", pattern = "csv")
archivos
#creo un df vacío
base<-data.frame()
for (i in archivos) {
df<-read.csv(paste0("./noticias/", i), encoding = "UTF-8")
base<-rbind(base, df)
}
#levanto todas las noticias en un solo df
for (i in archivos) {
df<-read.csv(paste0("./noticias/", i), encoding = "UTF-8")
base<-rbind(base, df)
}
#levanto todas las noticias en un solo df
for (i in archivos) {
df<-read.csv(paste0("./noticias/", i), encoding = "UTF-8")
base<-rbind(base, df)
}
#creo un df vacío
base<-data.frame()
#levanto todas las noticias en un solo df
for (i in archivos) {
df<-read.csv(paste0("./noticias/", i), encoding = "UTF-8")
base<-rbind(base, df)
}
# defino candidato
candidato<-"schiaretti"
#limpio textos de noticias y filtro las que mencionan al candidato
noticias<-base%>%
mutate(
texto= str_to_lower(texto),
texto= stri_trans_general(texto, "Latin - ASCII"),
texto= str_replace_all(texto,"www\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto,"https\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto, "[[:punct:]]", " "),
texto= str_replace_all(texto, "[[:digit:]]+", " "))%>%
filter(str_detect(texto, candidato))
View(noticias)
#creo un filtro de palabras que no quiero que salgan en el conteo
filtro_palabras<-c(stri_trans_general(stopwords::stopwords("es"), "Latin-ASCII"),
"patricia", "bullrich", "javier", "milei", "sergio", "massa", "myriam", "bregman", "schiaretti",
"anos", "dos")
stopwords::stopwords("es")
filtro_palabras
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)
View(palabras)
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)
View(palabras)
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())
View(palabras)
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))%>%
slice(1:150)
View(palabras)
write.table(palabras, file = paste0("./resultados/", candidato, "_nubedepalabras_", Sys.Date(),".csv"), quote = F, row.names = F, sep = ";")
library(tidyverse)
install.packages("Tidyverse")
install.packages("tidyverse")
install.packages("curl")
install.packages("jsonlite")
library(tidyverse)
library(curl)
library(jsonlite)
h<-new_handle()
handle_setheaders(h, usuario = "ariibardauil@gmail.com", token = "784283")
base<-curl_download("https://chaski.com.ar/api/noticias/20230810/20230817/clarin-lanacion-ambito-perfil/politica-economia-espectaculos", destfile = "datos.json", handle = h)
base<-curl_download("curl --location chaski.com.ar/noticias/20231009/20231011-16:20:00/clarin-paginadoce-perfil-lanacion-ambito-telam-infobae-eldestape-tn/politica", destfile = "datos.json", handle = h)
base<-curl_download("https://chaski.com.ar/noticias/20231009/20231011-16:20:00/clarin-paginadoce-perfil-lanacion-ambito-telam-infobae-eldestape-tn/politica", destfile = "datos.json", handle = h)
??curl}
# Creamos un nuevo "manejo" (handle) para las peticiones HTTP
h <- new_handle()
# Establecemos encabezados de autenticación
handle_setheaders(h, usuario = "ariibardauil@gmail.com", token = "784283")
# Especificamos la URL desde la cual descargaremos los datos
url <- "https://chaski.com.ar/noticias/20231009/20231011-16:20:00/clarin-paginadoce-perfil-lanacion-ambito-telam-infobae-eldestape-tn/politica"
# Nombre del archivo donde se guardará la respuesta de la petición HTTP
destfile <- "datos.json"
# Realizamos la descarga de los datos desde la URL
base <- curl_download(url, destfile = destfile, handle = h)
################################## Importo librerias ##############################
library(tidyverse)
library(curl)
library(jsonlite)
usuario ="joaquin.lovizio@gmail.com"
token = "09876"
# Armo funcion
query_noticias <- function(desde, hasta, medios, secciones, todos, alguno, ninguno, textual) {
h <- new_handle()
handle_setheaders(h, usuario = usuario, token = token)
# Construimos la URL con base a los parámetros de mas abajo
url <- paste0("https://chaski.com.ar/noticias/",
desde, "/",
hasta, "/",
paste(medios, collapse = "-"),
"/",
paste(secciones, collapse = "-"))
# Si son NULL no los incorpota y trae todos
if (!is.null(todos)) {
url <- paste0(url, "?todos=", URLencode(todos))
}
if (!is.null(alguno)) {
url <- paste0(url, "&alguno=", URLencode(alguno))
}
if (!is.null(ninguno)) {
url <- paste0(url, "&ninguno=", URLencode(ninguno))
}
if (!is.null(textual)) {
url <- paste0(url, "&textual=", URLencode(textual))
}
destfile <- "datos.json"
base <- curl_download(url, destfile = destfile, handle = h)
json <- fromJSON(destfile)
data <- as.data.frame(json$noticia)
return(data)
}
################################## Traigo noticias #############################################
# Parametros de la consulta
## Obligatorios
desde <- '20231007' # noticias DESDE esta fecha (se puede especificar horario: 20231005-10:30:00)
hasta <- '20231015'# noticias HASTA esta fecha (se puede especificar horario: 20231007-21:30:00)
medios <- c('clarin', 'lanacion', 'infobae', 'paginadoce', 'perfil', 'eldestape') # Diarios a filtrar
secciones <- NULL #c('politica', 'economia', 'sociedad', 'espectaculos')
todos <- NULL  #noticias que contengan TODOS estos términos
alguno <- NULL # noticias que contengan ALGUNO de estos términos
ninguno <- NULL # noticias que contengan NINGUNO de estos términos
textual <- NULL  #noticias que contengan EXACTAMENTE estos términos
# Corremos la función
notis <- query_noticias(desde, hasta, medios, secciones, todos, alguno, ninguno, textual)
print(paste0(desde,hasta,".csv",collapse = "-"))
print(paste(desde,hasta,".csv",collapse = "-"))
write.csv(notis,paste0(desde,hasta,".csv",collapse = "-"), row.names = FALSE)
print(paste(desde,hasta,".csv",sep = "-"))
print(paste("noticias",desde,hasta,".csv",sep = "-"))
write.csv(notis,paste("noticias/",desde,hasta,".csv",sep = "-"), row.names = FALSE)
write.csv(notis,paste("noticias/notis",desde,hasta,".csv",sep = "-"), row.names = FALSE)
source('ejemplos/ejemplo-chaski-en-R')
source('ejemplo-chaski-en-R')
source('ejemplo-chaski-en-R.R')
source('ejemplos/ejemplo-chaski-en-R.R')
library(tidyverse)
library(tidytext)
library(stringi)
source('ejemplos/ejemplo-chaski-en-R.R')
usuario ="joaquin.lovizio@gmail.com"
token = "09876"
################################## Traigo noticias #############################################
# Parametros de la consulta
## Obligatorios
desde <- '20231007' # noticias DESDE esta fecha (se puede especificar horario: 20231005-10:30:00)
hasta <- '20231015'# noticias HASTA esta fecha (se puede especificar horario: 20231007-21:30:00)
medios <- c('clarin', 'lanacion', 'infobae', 'paginadoce', 'perfil', 'eldestape') # Diarios a filtrar
secciones <- NULL #c('politica', 'economia', 'sociedad', 'espectaculos')
todos <- NULL  #noticias que contengan TODOS estos términos
alguno <- NULL # noticias que contengan ALGUNO de estos términos
ninguno <- NULL # noticias que contengan NINGUNO de estos términos
textual <- NULL  #noticias que contengan EXACTAMENTE estos términos
# Corremos la función
notis <- query_noticias(desde, hasta, medios, secciones, todos, alguno, ninguno, textual)
write.csv(notis,paste("noticias/notis",desde,hasta,".csv",sep = "-"), row.names = FALSE)
library(tidyverse)
library(tidytext)
library(stringi)
# Traigo la funcion de la libreria
source('libreria/ejemplo-chaski-en-R.R')
# Traigo la funcion de la libreria
source('libreria/chaski-en-R.R')
usuario ="joaquin.lovizio@gmail.com"
token = "09876"
################################## Traigo noticias #############################################
# Parametros de la consulta
## Obligatorios
desde <- '20231007' # noticias DESDE esta fecha (se puede especificar horario: 20231005-10:30:00)
hasta <- '20231015'# noticias HASTA esta fecha (se puede especificar horario: 20231007-21:30:00)
medios <- c('clarin', 'lanacion', 'infobae', 'paginadoce', 'perfil', 'eldestape') # Diarios a filtrar
secciones <- NULL #c('politica', 'economia', 'sociedad', 'espectaculos')
todos <- NULL  #noticias que contengan TODOS estos términos
alguno <- NULL # noticias que contengan ALGUNO de estos términos
ninguno <- NULL # noticias que contengan NINGUNO de estos términos
textual <- NULL  #noticias que contengan EXACTAMENTE estos términos
# Corremos la función
notis <- query_noticias(desde, hasta, medios, secciones, todos, alguno, ninguno, textual)
write.csv(notis,paste("noticias/notis",desde,hasta,".csv",sep = "-"), row.names = FALSE)
