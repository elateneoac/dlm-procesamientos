library(tidyverse)
library(tidytext)
library(stringi)
setwd(getwd())
list.files()
list.files("./data")
list.files("./noticias")
#defino los csv
list.files(path = "./noticias", pattern = "csv")
#defino los csv
archivos <- list.files(path = "./noticias", pattern = "csv")
#levanto todas las noticias en un solo df
for (i in archivos) {
df<-read.csv(i, encoding = "UTF-8")
base<-rbind(base, df)
}
df<-read.csv(paste0("./noticias/", i, encoding = "UTF-8")
base<-rbind(base, df)
}
noticias<-base%>%
mutate(
texto= str_to_lower(texto),
texto= stri_trans_general(texto, "Latin - ASCII"),
texto= str_replace_all(texto,"www\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto,"https\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto, "[[:punct:]]", " "),
texto= str_replace_all(texto, "[[:digit:]]+", " "))%>%
filter(str_detect(texto, "bullrich"))
filtro_palabras<-c(stri_trans_general(stopwords::stopwords("es"), "Latin-ASCII"),
"patricia", "milei", "massa", "bullrich", "javier")
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))
write.table(palabras, file = "palabras.csv", quote = F, row.names = F, sep = ";")
#levanto todas las noticias en un solo df
for (i in archivos) {
#levanto todas las noticias en un solo df
for (i in archivos) {
df<-read.csv(paste0("./noticias/", i, encoding = "UTF-8"))
base<-rbind(base, df)
}
df<-read.csv(paste0("./noticias/", i), encoding = "UTF-8")
#limpio textos de noticias y filtro las que mencionan al candidato
noticias<-base%>%
mutate(
texto= str_to_lower(texto),
texto= stri_trans_general(texto, "Latin - ASCII"),
texto= str_replace_all(texto,"www\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto,"https\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto, "[[:punct:]]", " "),
texto= str_replace_all(texto, "[[:digit:]]+", " "))%>%
filter(str_detect(texto, candidato))
# defino candidato
candidato<-"bullrich"
#limpio textos de noticias y filtro las que mencionan al candidato
noticias<-base%>%
mutate(
texto= str_to_lower(texto),
texto= stri_trans_general(texto, "Latin - ASCII"),
texto= str_replace_all(texto,"www\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto,"https\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto, "[[:punct:]]", " "),
texto= str_replace_all(texto, "[[:digit:]]+", " "))%>%
filter(str_detect(texto, candidato))
#creo un filtro de palabras que no quiero que salgan en el conteo
filtro_palabras<-c(stri_trans_general(stopwords::stopwords("es"), "Latin-ASCII"),
"patricia", "milei", "massa", "bullrich", "javier")
#hago el conteo de palabras
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))
#limpio textos de noticias y filtro las que mencionan al candidato
noticias<-base%>%
mutate(
texto= str_to_lower(texto),
texto= stri_trans_general(texto, "Latin - ASCII"),
texto= str_replace_all(texto,"www\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto,"https\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto, "[[:punct:]]", " "),
texto= str_replace_all(texto, "[[:digit:]]+", " "))%>%
filter(str_detect(texto, candidato))
#creo un df vacío
base<-data.frame()
#levanto todas las noticias en un solo df
for (i in archivos) {
df<-read.csv(paste0("./noticias/", i), encoding = "UTF-8")
base<-rbind(base, df)
}
# defino candidato
candidato<-"bullrich"
#limpio textos de noticias y filtro las que mencionan al candidato
noticias<-base%>%
mutate(
texto= str_to_lower(texto),
texto= stri_trans_general(texto, "Latin - ASCII"),
texto= str_replace_all(texto,"www\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto,"https\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto, "[[:punct:]]", " "),
texto= str_replace_all(texto, "[[:digit:]]+", " "))%>%
filter(str_detect(texto, candidato))
#creo un filtro de palabras que no quiero que salgan en el conteo
filtro_palabras<-c(stri_trans_general(stopwords::stopwords("es"), "Latin-ASCII"),
"patricia", "milei", "massa", "bullrich", "javier",
"anos", "dos")
#hago el conteo de palabras
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))
View(palabras)
write.table(palabras, file = paste0("./resultados/", candidato, "_nubedepalabras_", Sys.Date(),".csv"), quote = F, row.names = F, sep = ";")
#hago el conteo de palabras
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))%>%
slice(150)
write.table(palabras, file = paste0("./resultados/", candidato, "_nubedepalabras_", Sys.Date(),".csv"), quote = F, row.names = F, sep = ";")
#hago el conteo de palabras
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))%>%
slice(1:150)
write.table(palabras, file = paste0("./resultados/", candidato, "_nubedepalabras_", Sys.Date(),".csv"), quote = F, row.names = F, sep = ";")
# defino candidato
candidato<-"massa"
#limpio textos de noticias y filtro las que mencionan al candidato
noticias<-base%>%
mutate(
texto= str_to_lower(texto),
texto= stri_trans_general(texto, "Latin - ASCII"),
texto= str_replace_all(texto,"www\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto,"https\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto, "[[:punct:]]", " "),
texto= str_replace_all(texto, "[[:digit:]]+", " "))%>%
filter(str_detect(texto, candidato))
#creo un filtro de palabras que no quiero que salgan en el conteo
filtro_palabras<-c(stri_trans_general(stopwords::stopwords("es"), "Latin-ASCII"),
"patricia", "milei", "massa", "bullrich", "javier",
"anos", "dos")
#creo un filtro de palabras que no quiero que salgan en el conteo
filtro_palabras<-c(stri_trans_general(stopwords::stopwords("es"), "Latin-ASCII"),
"patricia", "milei", "massa", "bullrich", "javier",
"anos", "dos")
#hago el conteo de palabras
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))%>%
slice(1:150)
write.table(palabras, file = paste0("./resultados/", candidato, "_nubedepalabras_", Sys.Date(),".csv"), quote = F, row.names = F, sep = ";")
# defino candidato
candidato<-"bregman"
#limpio textos de noticias y filtro las que mencionan al candidato
noticias<-base%>%
mutate(
texto= str_to_lower(texto),
texto= stri_trans_general(texto, "Latin - ASCII"),
texto= str_replace_all(texto,"www\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto,"https\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto, "[[:punct:]]", " "),
texto= str_replace_all(texto, "[[:digit:]]+", " "))%>%
filter(str_detect(texto, candidato))
#creo un filtro de palabras que no quiero que salgan en el conteo
filtro_palabras<-c(stri_trans_general(stopwords::stopwords("es"), "Latin-ASCII"),
"patricia", "milei", "massa", "bullrich", "javier",
"anos", "dos")
#hago el conteo de palabras
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))%>%
slice(1:150)
write.table(palabras, file = paste0("./resultados/", candidato, "_nubedepalabras_", Sys.Date(),".csv"), quote = F, row.names = F, sep = ";")
library(tidyverse)
library(tidytext)
library(stringi)
getwd()
getwd()
getwd()
setwd(getwd())
#defino los csv
archivos <- list.files(path = "./noticias", pattern = "csv")
archivos
#creo un df vacío
base<-data.frame()
for (i in archivos) {
df<-read.csv(paste0("./noticias/", i), encoding = "UTF-8")
base<-rbind(base, df)
}
#levanto todas las noticias en un solo df
for (i in archivos) {
df<-read.csv(paste0("./noticias/", i), encoding = "UTF-8")
base<-rbind(base, df)
}
#levanto todas las noticias en un solo df
for (i in archivos) {
df<-read.csv(paste0("./noticias/", i), encoding = "UTF-8")
base<-rbind(base, df)
}
#creo un df vacío
base<-data.frame()
#levanto todas las noticias en un solo df
for (i in archivos) {
df<-read.csv(paste0("./noticias/", i), encoding = "UTF-8")
base<-rbind(base, df)
}
# defino candidato
candidato<-"schiaretti"
#limpio textos de noticias y filtro las que mencionan al candidato
noticias<-base%>%
mutate(
texto= str_to_lower(texto),
texto= stri_trans_general(texto, "Latin - ASCII"),
texto= str_replace_all(texto,"www\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto,"https\\S*", ""), #saco urls con REGEX
texto= str_replace_all(texto, "[[:punct:]]", " "),
texto= str_replace_all(texto, "[[:digit:]]+", " "))%>%
filter(str_detect(texto, candidato))
View(noticias)
#creo un filtro de palabras que no quiero que salgan en el conteo
filtro_palabras<-c(stri_trans_general(stopwords::stopwords("es"), "Latin-ASCII"),
"patricia", "bullrich", "javier", "milei", "sergio", "massa", "myriam", "bregman", "schiaretti",
"anos", "dos")
stopwords::stopwords("es")
filtro_palabras
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)
View(palabras)
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)
View(palabras)
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())
View(palabras)
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))
palabras<-noticias%>%
unnest_tokens(input = texto, output = word)%>%
filter(!word %in% filtro_palabras)%>%
group_by(word)%>%
summarise(cantidad = n())%>%
arrange(desc(cantidad))%>%
slice(1:150)
View(palabras)
write.table(palabras, file = paste0("./resultados/", candidato, "_nubedepalabras_", Sys.Date(),".csv"), quote = F, row.names = F, sep = ";")
library(tidyverse)
install.packages("Tidyverse")
install.packages("tidyverse")
install.packages("curl")
install.packages("jsonlite")
library(tidyverse)
library(curl)
library(jsonlite)
h<-new_handle()
handle_setheaders(h, usuario = "ariibardauil@gmail.com", token = "784283")
base<-curl_download("https://chaski.com.ar/api/noticias/20230810/20230817/clarin-lanacion-ambito-perfil/politica-economia-espectaculos", destfile = "datos.json", handle = h)
base<-curl_download("curl --location chaski.com.ar/noticias/20231009/20231011-16:20:00/clarin-paginadoce-perfil-lanacion-ambito-telam-infobae-eldestape-tn/politica", destfile = "datos.json", handle = h)
base<-curl_download("https://chaski.com.ar/noticias/20231009/20231011-16:20:00/clarin-paginadoce-perfil-lanacion-ambito-telam-infobae-eldestape-tn/politica", destfile = "datos.json", handle = h)
??curl}
# Creamos un nuevo "manejo" (handle) para las peticiones HTTP
h <- new_handle()
# Establecemos encabezados de autenticación
handle_setheaders(h, usuario = "ariibardauil@gmail.com", token = "784283")
# Especificamos la URL desde la cual descargaremos los datos
url <- "https://chaski.com.ar/noticias/20231009/20231011-16:20:00/clarin-paginadoce-perfil-lanacion-ambito-telam-infobae-eldestape-tn/politica"
# Nombre del archivo donde se guardará la respuesta de la petición HTTP
destfile <- "datos.json"
# Realizamos la descarga de los datos desde la URL
base <- curl_download(url, destfile = destfile, handle = h)
